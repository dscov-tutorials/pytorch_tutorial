{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch-MNIST-Example.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gFi_uyifwzfX","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nbrs2cTGFUXf","colab_type":"text"},"source":["**Define Model**\n","\n","In PyTorch, your models should also subclass `torch.nn.Module` class.\n","\n","In this tutorial, we will build a convolutional neural network for image classification.\n","\n","![alt text](https://miro.medium.com/max/2510/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n","\n"]},{"cell_type":"code","metadata":{"id":"Bb5KNQKww__0","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        In the constructor we instantiate two nn.Conv2d and nn.Linear modules and assign them as\n","        member variables.\n","        \"\"\"\n","        super(Net, self).__init__()\n","        \n","        num_classes = 10\n","        \n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n","        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n","        self.fc1 = nn.Linear(in_features=4*4*50, out_features=500)\n","        self.fc2 = nn.Linear(in_features=500, out_features=num_classes)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        In the forward function we accept a Tensor of input data and we must return\n","        a Tensor of output data. We can use Modules defined in the constructor as\n","        well as arbitrary operators on Tensors.\n","        \"\"\"\n","        \n","        x = F.relu(self.conv1(x))  # [batchx1x28x28] --> [batchx20x24x24]\n","        x = F.max_pool2d(x, 2, 2)  # [batchx20x24x24] --> [batchx20x12x12]\n","        x = F.relu(self.conv2(x))  # [batchx20x12x12] --> [batchx50x8x8]\n","        x = F.max_pool2d(x, 2, 2)  # [batchx50x8x8] --> [batchx50x4x4]\n","        x = x.view(-1, 4*4*50)     # flatten 3D tensor to 1D\n","        x = F.relu(self.fc1(x))    # [batchx(50x4x4)] --> [batchx500]\n","        x = self.fc2(x)            # [batchx500] --> [batchx10]\n","        return F.log_softmax(x, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IftX50blxyh6","colab_type":"code","outputId":"c942bd5f-e636-4243-9e7e-cafdfe6250d6","executionInfo":{"status":"ok","timestamp":1571417682273,"user_tz":240,"elapsed":167,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["model = Net()\n","print(model)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=800, out_features=500, bias=True)\n","  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"br1fXMI2yTrr","colab_type":"code","outputId":"9a4ce4a7-ef78-4d22-94f1-d5d4dc6ef1b7","executionInfo":{"status":"ok","timestamp":1571417706800,"user_tz":240,"elapsed":171,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["print(model.conv2)\n","print(model.conv2.weight.shape)\n","print(model.conv2.weight.device)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n","torch.Size([50, 20, 5, 5])\n","cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ujPN_ActLEvZ","colab_type":"text"},"source":["**Set Hyperparameters**"]},{"cell_type":"code","metadata":{"id":"pg93Cj1GKZE4","colab_type":"code","outputId":"600d01f7-bcd5-436b-f037-45e35f28ec38","executionInfo":{"status":"ok","timestamp":1571417763036,"user_tz":240,"elapsed":173,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import argparse\n","\n","parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n","parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n","                    help='input batch size for training (default: 64)')\n","parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n","                    help='input batch size for testing (default: 1000)')\n","parser.add_argument('--epochs', type=int, default=3, metavar='N',\n","                    help='number of epochs to train (default: 10)')\n","parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n","                    help='learning rate (default: 0.01)')\n","parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n","                    help='SGD momentum (default: 0.5)')\n","parser.add_argument('--no-cuda', action='store_true', default=False,\n","                    help='disables CUDA training')\n","parser.add_argument('--seed', type=int, default=1, metavar='S',\n","                    help='random seed (default: 1)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","                    help='how many batches to wait before logging training status')\n","\n","parser.add_argument('--save-model', action='store_true', default=False,\n","                    help='For Saving the current Model')\n","\n","args = parser.parse_args(args=['--save-model'])\n","print(args)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Namespace(batch_size=64, epochs=3, log_interval=10, lr=0.01, momentum=0.5, no_cuda=False, save_model=True, seed=1, test_batch_size=1000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-sW3pw0rXxNU","colab_type":"text"},"source":["**Fix Random Seed**"]},{"cell_type":"code","metadata":{"id":"fRTrkZsQXxcC","colab_type":"code","outputId":"3a041eb3-6b87-4b71-e832-c2aeb9fd758e","executionInfo":{"status":"ok","timestamp":1571417767171,"user_tz":240,"elapsed":191,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.manual_seed(args.seed)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd5d07685b0>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"GR4y5gEFQhPu","colab_type":"code","outputId":"6ae5d362-5679-408f-f3d5-07c95323b44d","executionInfo":{"status":"ok","timestamp":1571417769710,"user_tz":240,"elapsed":179,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch.optim as optim\n","\n","use_cuda = not args.no_cuda and torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","model = model.to(device)\n","optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","print(model.conv2.weight.device)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N3WU748wL2Sg","colab_type":"text"},"source":["**Define Dataset and DataLoader**\n","\n","MNIST handwritten digit dataset\n","\n","![alt text](https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png)"]},{"cell_type":"code","metadata":{"id":"yB4ZNaHDYEbL","colab_type":"code","outputId":"c80c26e9-c40a-4e04-8f3d-c593f8468df1","executionInfo":{"status":"ok","timestamp":1571341691870,"user_tz":240,"elapsed":161,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"09046723808353533857"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rJCojEHqJnia","colab_type":"code","outputId":"3c356e5f-6647-4d58-a8df-223f870fb0c9","executionInfo":{"status":"ok","timestamp":1571413427227,"user_tz":240,"elapsed":4330,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":263}},"source":["from torchvision import datasets, transforms\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./dataset', train=True, download=True,\n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","    batch_size=args.batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./dataset', train=False, transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","    batch_size=args.test_batch_size, shuffle=True, **kwargs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["9920512it [00:00, 20136150.37it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 330079.79it/s]\n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n","Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["1654784it [00:00, 5938665.77it/s]                           \n","8192it [00:00, 113537.12it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-ULcqiTFHXeU","colab_type":"text"},"source":["**Train Phase**\n","\n","Set your model in training mode (this has any effect only on certain modules, e.g. Dropout, BatchNorm, etc.)\n","\n","![alt text](https://blog.exxactcorp.com/wp-content/uploads/2019/06/tensors_flowing.gif)"]},{"cell_type":"code","metadata":{"id":"VhLN5VG1xtOD","colab_type":"code","colab":{}},"source":["def train(args, model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nXLsUVnfHNzj","colab_type":"text"},"source":["**Test Phase**\n","\n","Set your model in evaluation mode (this has any effect only on certain modules, e.g. `Dropout`, `BatchNorm`, etc.)"]},{"cell_type":"code","metadata":{"id":"dy-znIJax5fw","colab_type":"code","colab":{}},"source":["def test(args, model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TSPzR0LZKObu","colab_type":"text"},"source":["**Start Training and Testing**"]},{"cell_type":"code","metadata":{"id":"B8SIT7O0zces","colab_type":"code","outputId":"f4bfdc39-e5ff-49a2-fd0b-e72e0bc90962","executionInfo":{"status":"ok","timestamp":1571418134276,"user_tz":240,"elapsed":63009,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for epoch in range(1, args.epochs + 1):\n","    train(args, model, device, train_loader, optimizer, epoch)\n","    test(args, model, device, test_loader)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.279750\n","Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.254422\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.185086\n","Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.131899\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.983129\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.678403\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.371962\n","Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.034748\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.687925\n","Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.476121\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.484266\n","Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.621842\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.565654\n","Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.753579\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.408918\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.462027\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.352636\n","Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.224936\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.234863\n","Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.301127\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.466136\n","Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.301700\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.338057\n","Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.391922\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.252105\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.489909\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.355604\n","Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.405219\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.268954\n","Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.212080\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.374139\n","Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.223594\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.479981\n","Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.259493\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.342665\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.110959\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.238383\n","Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.323072\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.185243\n","Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.139629\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.332373\n","Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.189224\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.148011\n","Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.229905\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.101309\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.168582\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.188345\n","Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.129447\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.327626\n","Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.201021\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.134791\n","Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.204414\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.123247\n","Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.103335\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.095892\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.096284\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.150085\n","Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.250326\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.078205\n","Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.177036\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.133918\n","Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.117064\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.107721\n","Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.200387\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.106603\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.100270\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.061954\n","Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.138273\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.089604\n","Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.066375\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.065342\n","Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.075133\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.117059\n","Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.125281\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.192370\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.139961\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.171679\n","Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.054984\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.072004\n","Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.071030\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.151695\n","Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.064594\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.274875\n","Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.170681\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.084210\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.073004\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.105641\n","Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.123273\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.048880\n","Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.039698\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093747\n","Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.126575\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.119487\n","Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.032188\n","\n","Test set: Average loss: 0.1151, Accuracy: 9633/10000 (96%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.090739\n","Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.101246\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.124345\n","Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.132133\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.076252\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.065978\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.074323\n","Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.028466\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.064387\n","Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.076830\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.026555\n","Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.061482\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.045337\n","Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.117514\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.098479\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.069978\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.131573\n","Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.167817\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.052070\n","Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.105939\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.082791\n","Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.131969\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.169386\n","Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.085661\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.082314\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.077758\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.054413\n","Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.175241\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.100038\n","Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.052907\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.032154\n","Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.043948\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.045010\n","Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.128146\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.049604\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.057169\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.040252\n","Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.118589\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.057464\n","Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.125255\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.036903\n","Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.053931\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.102203\n","Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.060856\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.079552\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.125464\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.013092\n","Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.091501\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.010845\n","Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.080361\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.102366\n","Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.097097\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.069186\n","Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.131546\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.032745\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.055339\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.132824\n","Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.070332\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.044402\n","Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.176435\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.035345\n","Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.062024\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.052974\n","Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.076700\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.071112\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.191047\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.057734\n","Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.065413\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.134027\n","Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.045831\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.118536\n","Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.025087\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.087674\n","Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.107545\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.087697\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.176371\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.068726\n","Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.022470\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.014360\n","Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.042495\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.090178\n","Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.047659\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.124189\n","Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.050246\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.020049\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.038396\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.017489\n","Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.056761\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.067659\n","Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.049155\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090500\n","Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.059080\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.024898\n","Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.037367\n","\n","Test set: Average loss: 0.0616, Accuracy: 9812/10000 (98%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.114604\n","Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.012927\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.031534\n","Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.133085\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.182272\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.024013\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.115612\n","Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.050641\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.043688\n","Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.111397\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.095078\n","Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.044002\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.138549\n","Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.044615\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.013114\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.096530\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.050143\n","Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.081074\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.044991\n","Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.114950\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.093178\n","Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.067430\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.105216\n","Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.023780\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.088181\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.006249\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.127593\n","Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.020145\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.104227\n","Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.028217\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.059104\n","Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.032395\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.072346\n","Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.013266\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.034660\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.045851\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.037724\n","Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.067455\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.045006\n","Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.013384\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.198579\n","Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.080368\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.024462\n","Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.126050\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.026357\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.092042\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.035655\n","Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.033561\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.033899\n","Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.011375\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.029205\n","Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.007222\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.058927\n","Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.111863\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.081969\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.046748\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.016592\n","Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.032053\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.067793\n","Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.017769\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.041177\n","Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.009624\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.035616\n","Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.172226\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.079404\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.034567\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.023521\n","Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.008990\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.067101\n","Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.081859\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.089701\n","Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.230232\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.031148\n","Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.067405\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.034560\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.022047\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.004823\n","Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.009206\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.049671\n","Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.070712\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.031339\n","Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.100573\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.071877\n","Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.055425\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.054719\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.023494\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.107438\n","Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.020484\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.123748\n","Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.124004\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.029756\n","Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.080601\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.091824\n","Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.207756\n","\n","Test set: Average loss: 0.0515, Accuracy: 9835/10000 (98%)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JjedtRD0NkBR","colab_type":"text"},"source":["**Save the trained model**\n","\n","A `state_dict` is simply a Python dictionary object that maps each layer to its parameter tensor.\n","\n","Because `state_dict` objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers.\n","\n","When saving a model for inference, it is only necessary to save the trained model’s learned parameters. Saving the model’s `state_dict` with the `torch.save()` function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\n","\n","(https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"ZQaioYSGNJXJ","colab_type":"code","outputId":"c2cf3309-6cdf-4779-a506-33ac88fc1b45","executionInfo":{"status":"ok","timestamp":1571418149151,"user_tz":240,"elapsed":187,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"source":["# Print model's state_dict\n","print(\"Model's state_dict:\")\n","for param_tensor in model.state_dict():\n","    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","    \n","if (args.save_model):\n","  torch.save(model.state_dict(),\"mnist_cnn.pt\")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Model's state_dict:\n","conv1.weight \t torch.Size([20, 1, 5, 5])\n","conv1.bias \t torch.Size([20])\n","conv2.weight \t torch.Size([50, 20, 5, 5])\n","conv2.bias \t torch.Size([50])\n","fc1.weight \t torch.Size([500, 800])\n","fc1.bias \t torch.Size([500])\n","fc2.weight \t torch.Size([10, 500])\n","fc2.bias \t torch.Size([10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ca1udkG9N2Us","colab_type":"text"},"source":["**Load the pre-trained model**"]},{"cell_type":"code","metadata":{"id":"lMAksv9qN5wU","colab_type":"code","outputId":"4babecd3-e839-4c7c-8b68-3d738a108c73","executionInfo":{"status":"ok","timestamp":1571418176036,"user_tz":240,"elapsed":4712,"user":{"displayName":"Minju Jung","photoUrl":"","userId":"12016452202641329507"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["new_model = Net()\n","new_model = new_model.to(device)\n","\n","# test the model with random initialization\n","test(args, new_model, device, test_loader)\n","\n","# load the pre-trained parameters\n","new_model.load_state_dict(torch.load('mnist_cnn.pt'))\n","\n","# test the pre-trained model\n","test(args, new_model, device, test_loader)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["\n","Test set: Average loss: 2.3093, Accuracy: 1109/10000 (11%)\n","\n","\n","Test set: Average loss: 0.0515, Accuracy: 9835/10000 (98%)\n","\n"],"name":"stdout"}]}]}